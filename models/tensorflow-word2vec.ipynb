{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook compare gensim Word2Vec implementation with the self-build TensorFlow Word2Vec model by a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Directly Use Word2ec model in gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose the skip-gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1: pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is the king . the king is royal . she is the royal queen . she is the queen \n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# set the corpus, divide it into sentences.\n",
    "corpus_raw = 'He is the king . The king is royal . She is the royal queen . She is the queen '\n",
    "\n",
    "# convert to lower case\n",
    "corpus_raw = corpus_raw.lower()\n",
    "print(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'the', 'king'], ['the', 'king', 'is', 'royal'], ['she', 'is', 'the', 'royal', 'queen'], ['she', 'is', 'the', 'queen']]\n"
     ]
    }
   ],
   "source": [
    "# sentences list for training model\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:  {'royal', 'she', 'is', 'queen', 'king', 'he', 'the'}\n",
      "word2int:  {'royal': 0, 'she': 1, 'is': 2, 'queen': 3, 'king': 4, 'he': 5, 'the': 6}\n",
      "int2word:  {0: 'royal', 1: 'she', 2: 'is', 3: 'queen', 4: 'king', 5: 'he', 6: 'the'}\n"
     ]
    }
   ],
   "source": [
    "# remove . and split the sentence\n",
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.': # because we don't want to treat . as a word\n",
    "        words.append(word)\n",
    "        \n",
    "# create word2int an int2word dictionaries\n",
    "words = set(words) # so that all duplicate words are removed\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words) # gives the total number of unique words\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "\n",
    "print(\"words: \", words)\n",
    "print(\"word2int: \", word2int)\n",
    "print(\"int2word: \", int2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the corpus to train a word2vec model\n",
    "model = Word2Vec(sentences, window=2, min_count=1, size=5, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 3: find the closest word / visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the utility functions\n",
    "# calculate the euclidean distance\n",
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
    "\n",
    "# find the closest embedding\n",
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04108642 -0.08220293 -0.08155387 -0.06814617  0.08490859]\n",
      " [-0.09877158 -0.06708763  0.03907226 -0.03416926 -0.04674773]\n",
      " [ 0.01337672  0.04916007 -0.00382072  0.06348719 -0.05318097]\n",
      " [-0.07780853 -0.01206629  0.02227198 -0.09675884 -0.03171881]\n",
      " [-0.03523685 -0.03847206  0.05936517  0.01271019 -0.01490353]\n",
      " [-0.02868939  0.0151211  -0.08545563 -0.07397427  0.02091852]\n",
      " [-0.02190888  0.07080114  0.03469391 -0.03839784  0.08377661]]\n"
     ]
    }
   ],
   "source": [
    "# create an array to save all the word embeddings\n",
    "import numpy as np\n",
    "vectors = np.empty((0,5), int)\n",
    "for i in word2int:\n",
    "    v = model.wv[i].reshape(1,5)\n",
    "    vectors = np.append(vectors, v, axis=0)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the closest word to king is: she\n"
     ]
    }
   ],
   "source": [
    "# predict the closest word\n",
    "input_word = \"king\"\n",
    "word_index = word2int[input_word]\n",
    "output_word = int2word[find_closest(word_index, vectors)]\n",
    "print(\"the closest word to\", input_word, \"is:\", output_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to get the decomposition of the word embeddings\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(vectors)\n",
    "vectors = pca.transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'royal', 'she', 'is', 'queen', 'king', 'he', 'the'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the word embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "print(words)\n",
    "for word in words:\n",
    "    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\n",
    "    ax.set_xlim(min([vectors[word2int[w]][0] for w in words])-0.1, max([vectors[word2int[w]][0] for w in words])+0.1)\n",
    "    ax.set_ylim(min([vectors[word2int[w]][1] for w in words])-0.1, max([vectors[word2int[w]][1] for w in words])+0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. build Word2Vec step by step in TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is the king . the king is royal . she is the royal queen . she is the queen \n"
     ]
    }
   ],
   "source": [
    "# define a corpus\n",
    "corpus_raw = 'He is the king . The king is royal . She is the royal queen . She is the queen '\n",
    "\n",
    "# convert to lower case\n",
    "corpus_raw = corpus_raw.lower()\n",
    "print(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'the', 'king', 'the', 'king', 'is', 'royal', 'she', 'is', 'the', 'royal', 'queen', 'she', 'is', 'the', 'queen']\n"
     ]
    }
   ],
   "source": [
    "# remove . and split the sentence\n",
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.': # because we don't want to treat . as a word\n",
    "        words.append(word)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0, 'he': 1, 'royal': 2, 'the': 3, 'she': 4, 'king': 5, 'queen': 6}\n",
      "{0: 'is', 1: 'he', 2: 'royal', 3: 'the', 4: 'she', 5: 'king', 6: 'queen'}\n"
     ]
    }
   ],
   "source": [
    "# create word2int an int2word dictionaries\n",
    "words = set(words) # so that all duplicate words are removed\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words) # gives the total number of unique words\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "print(word2int)\n",
    "print(int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'the', 'king'], ['the', 'king', 'is', 'royal'], ['she', 'is', 'the', 'royal', 'queen'], ['she', 'is', 'the', 'queen']]\n"
     ]
    }
   ],
   "source": [
    "# raw sentences is a list of sentences.\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is'], ['he', 'the'], ['is', 'he'], ['is', 'the'], ['is', 'king'], ['the', 'he'], ['the', 'is'], ['the', 'king'], ['king', 'is'], ['king', 'the'], ['the', 'king'], ['the', 'is'], ['king', 'the'], ['king', 'is'], ['king', 'royal'], ['is', 'the'], ['is', 'king'], ['is', 'royal'], ['royal', 'king'], ['royal', 'is'], ['she', 'is'], ['she', 'the'], ['is', 'she'], ['is', 'the'], ['is', 'royal'], ['the', 'she'], ['the', 'is'], ['the', 'royal'], ['the', 'queen'], ['royal', 'is'], ['royal', 'the'], ['royal', 'queen'], ['queen', 'the'], ['queen', 'royal'], ['she', 'is'], ['she', 'the'], ['is', 'she'], ['is', 'the'], ['is', 'queen'], ['the', 'she'], ['the', 'is'], ['the', 'queen'], ['queen', 'is'], ['queen', 'the']]\n"
     ]
    }
   ],
   "source": [
    "# prepare word-neighborWord pair\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "data = []\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "y_train\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# prepare training data\n",
    "x_train = [] # input word\n",
    "y_train = [] # output word (neighbor word)\n",
    "\n",
    "for data_word in data:\n",
    "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n",
    "\n",
    "# convert them to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "print(\"x_train\")\n",
    "print(x_train)\n",
    "print(\"y_train\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: define the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss is: 5.02179\n",
      "epoch: 1000 loss is: 1.3840604\n"
     ]
    }
   ],
   "source": [
    "# making placeholders for x_train and y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "\n",
    "EMBEDDING_DIM = 5 # you can choose your own number\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) # sometimes we don't have bias here\n",
    "hidden_representation = tf.add(tf.matmul(x,W1), b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))\n",
    "\n",
    "# initialize the session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) # make sure you do this!\n",
    "\n",
    "# define the loss function (actually the newest Word2Vec model is using more efficient loss function)\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
    "\n",
    "# define the training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "\n",
    "# define training iterations/epochs\n",
    "n_iters = 2000\n",
    "\n",
    "# train for n_iter iterations\n",
    "for n in range(n_iters):\n",
    "    _, loss = sess.run([train_step, cross_entropy_loss], feed_dict={x: x_train, y_label: y_train})\n",
    "    if n % 1000 == 0:\n",
    "        print('epoch:', n, 'loss is:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43957722 -0.02169335  0.99495244  0.7440139   0.30535778]\n",
      " [ 0.93303806  2.5875502  -1.2420619  -0.61183727  1.3030291 ]\n",
      " [ 0.9251915   0.18413424  0.05256362 -2.085824   -0.70353365]\n",
      " [-0.7467028   0.25536013 -0.2696684   0.745507   -2.1591544 ]\n",
      " [ 1.9636409   1.880081   -0.40516308  0.41972914 -0.5456779 ]\n",
      " [ 0.01716292  3.944802    0.6797297  -0.4324559  -0.28972468]\n",
      " [-0.16922033  2.72589     0.445101   -0.23763134 -0.03180511]]\n"
     ]
    }
   ],
   "source": [
    "# get the embeddings of all the words\n",
    "vectors = sess.run(W1 + b1)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the euclidean distance\n",
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
    "\n",
    "# find the closest embedding\n",
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the closest word to king is: queen\n"
     ]
    }
   ],
   "source": [
    "# predict the closest word\n",
    "input_word = \"king\"\n",
    "word_index = word2int[input_word]\n",
    "output_word = int2word[find_closest(word_index, vectors)]\n",
    "print(\"the closest word to\", input_word, \"is:\", output_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: visulization of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(vectors)\n",
    "vectors = pca.transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is', 'he', 'royal', 'the', 'she', 'king', 'queen'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "print(words)\n",
    "for word in words:\n",
    "#     print(word, vectors[word2int[word]][1])\n",
    "    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\n",
    "    ax.set_xlim(min([vectors[word2int[w]][0] for w in words])-1, max([vectors[word2int[w]][0] for w in words])+1)\n",
    "    ax.set_ylim(min([vectors[word2int[w]][1] for w in words])-1, max([vectors[word2int[w]][1] for w in words])+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1-test",
   "language": "python",
   "name": "tf1-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
